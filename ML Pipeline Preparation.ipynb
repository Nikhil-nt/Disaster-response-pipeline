{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Nik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import sqlite3 \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "df = pd.read_sql('Select * from df1',engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.message.values\n",
    "Y = df[['aid_centers', 'aid_related', 'buildings', 'child_alone', 'clothing',\n",
    "       'cold', 'death', 'direct_report', 'earthquake', 'electricity', 'fire',\n",
    "       'floods', 'food', 'hospitals', 'infrastructure_related',\n",
    "       'medical_help', 'medical_products', 'military',\n",
    "       'missing_people', 'money', 'offer', 'other_aid',\n",
    "       'other_infrastructure', 'other_weather', 'refugees', 'related',\n",
    "       'request', 'search_and_rescue', 'security', 'shelter', 'shops', 'storm',\n",
    "       'tools', 'transport', 'water', 'weather_related']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "def tokenize(text):\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    \n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "    return clean_tokens\n",
    "\n",
    "            \n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather update - a cold front from Cuba that could pass over Haiti\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['weather',\n",
       " 'update',\n",
       " '-',\n",
       " 'a',\n",
       " 'cold',\n",
       " 'front',\n",
       " 'from',\n",
       " 'cuba',\n",
       " 'that',\n",
       " 'could',\n",
       " 'pas',\n",
       " 'over',\n",
       " 'haiti']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X[0])\n",
    "tokenize(str(X[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "vect = CountVectorizer()\n",
    "tfidf = TfidfTransformer()\n",
    "clf = MultiOutputClassifier(vect)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier()))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y)\n",
    "pipeline.fit(X_train,y_train)\n",
    "y_pred=pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        63\n",
      "           1       0.76      0.52      0.62      2698\n",
      "           2       0.86      0.10      0.18       322\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.75      0.03      0.06        95\n",
      "           5       0.75      0.18      0.29       132\n",
      "           6       0.70      0.11      0.19       276\n",
      "           7       0.78      0.32      0.45      1244\n",
      "           8       0.90      0.59      0.72       601\n",
      "           9       0.70      0.05      0.09       139\n",
      "          10       0.00      0.00      0.00        63\n",
      "          11       0.93      0.28      0.44       558\n",
      "          12       0.83      0.27      0.41       693\n",
      "          13       1.00      0.01      0.02        83\n",
      "          14       0.20      0.00      0.00       436\n",
      "          15       0.63      0.07      0.13       523\n",
      "          16       0.73      0.07      0.14       321\n",
      "          17       0.65      0.05      0.09       222\n",
      "          18       0.00      0.00      0.00        68\n",
      "          19       0.67      0.04      0.07       166\n",
      "          20       0.00      0.00      0.00        28\n",
      "          21       0.54      0.04      0.07       821\n",
      "          22       0.00      0.00      0.00       290\n",
      "          23       0.50      0.01      0.02       366\n",
      "          24       0.50      0.01      0.02       229\n",
      "          25       0.82      0.94      0.87      4989\n",
      "          26       0.82      0.37      0.51      1057\n",
      "          27       0.62      0.03      0.05       187\n",
      "          28       1.00      0.01      0.02       107\n",
      "          29       0.81      0.22      0.35       578\n",
      "          30       0.00      0.00      0.00        39\n",
      "          31       0.74      0.27      0.40       635\n",
      "          32       0.00      0.00      0.00        30\n",
      "          33       0.66      0.06      0.11       305\n",
      "          34       0.81      0.26      0.39       405\n",
      "          35       0.87      0.56      0.68      1856\n",
      "\n",
      "   micro avg       0.81      0.45      0.58     20625\n",
      "   macro avg       0.57      0.15      0.21     20625\n",
      "weighted avg       0.75      0.45      0.50     20625\n",
      " samples avg       0.67      0.43      0.48     20625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "\n",
    "for columns in df.columns:\n",
    "    \n",
    "    print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'SVC' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-b9ae3b14573a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mgrid_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mbest_clf\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mgrid_fit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\New folder\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, estimator, param_grid, scoring, fit_params, n_jobs, iid, refit, cv, verbose, pre_dispatch, error_score, return_train_score)\u001b[0m\n\u001b[0;32m   1185\u001b[0m             return_train_score=return_train_score)\n\u001b[0;32m   1186\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1187\u001b[1;33m         \u001b[0m_check_param_grid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\New folder\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_check_param_grid\u001b[1;34m(param_grid)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'SVC' object is not iterable"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer,f1_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# TODO: Make an fbeta_score scoring object.\n",
    "scorer = make_scorer(f1_score)\n",
    "parameters =  SVC(kernel='linear',random_state=0)\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters,scoring=scorer)\n",
    "grid_fit = cv.fit(X_train,y_train)\n",
    "best_clf =  grid_fit.best_estimator_\n",
    "best_clf.fit(X_train,y_train)\n",
    "best_train_predictions = best_clf.predict(X_train)\n",
    "best_test_predictions = best_clf.predict(X_test)\n",
    "print('The training F1 Score is', f1_score(best_train_predictions, y_pred))\n",
    "print('The testing F1 Score is', f1_score(best_test_predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (y_pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9415496897568915"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# initialize count vectorizer object\n",
    "vect = CountVectorizer(tokenizer=tokenize)\n",
    "# get counts of each token (word) in text data\n",
    "X = vect.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-5238e41eaf4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Downloads\\New folder\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    945\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\New folder\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 1, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib \n",
    "  \n",
    "# Save the model as a pickle in a file \n",
    "joblib.dump(pipeline, 'filename.pkl') \n",
    "  \n",
    "# Load the model from the file \n",
    "pipeline_from_joblib = joblib.load('filename.pkl')  \n",
    "  \n",
    "# Use the loaded model to make predictions \n",
    "pipeline_from_joblib.predict(X_test) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

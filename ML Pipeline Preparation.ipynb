{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Nik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import sqlite3 \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "df = pd.read_sql('Select * from df1',engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.message.values\n",
    "Y = df[['aid_centers', 'aid_related', 'buildings', 'child_alone', 'clothing',\n",
    "       'cold', 'death', 'direct_report', 'earthquake', 'electricity', 'fire',\n",
    "       'floods', 'food', 'hospitals', 'infrastructure_related',\n",
    "       'medical_help', 'medical_products', 'military',\n",
    "       'missing_people', 'money', 'offer', 'other_aid',\n",
    "       'other_infrastructure', 'other_weather', 'refugees', 'related',\n",
    "       'request', 'search_and_rescue', 'security', 'shelter', 'shops', 'storm',\n",
    "       'tools', 'transport', 'water', 'weather_related']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "def tokenize(text):\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    \n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "    return clean_tokens\n",
    "\n",
    "            \n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather update - a cold front from Cuba that could pass over Haiti\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['weather',\n",
       " 'update',\n",
       " '-',\n",
       " 'a',\n",
       " 'cold',\n",
       " 'front',\n",
       " 'from',\n",
       " 'cuba',\n",
       " 'that',\n",
       " 'could',\n",
       " 'pas',\n",
       " 'over',\n",
       " 'haiti']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X[0])\n",
    "tokenize(str(X[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "vect = CountVectorizer()\n",
    "tfidf = TfidfTransformer()\n",
    "clf = MultiOutputClassifier(vect)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier()))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y)\n",
    "pipeline.fit(X_train,y_train)\n",
    "y_pred=pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Nik\\Downloads\\New folder\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        87\n",
      "           1       0.75      0.52      0.61      2703\n",
      "           2       0.61      0.06      0.11       333\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.83      0.16      0.27        94\n",
      "           5       0.80      0.11      0.19       148\n",
      "           6       0.75      0.11      0.19       302\n",
      "           7       0.80      0.31      0.45      1215\n",
      "           8       0.88      0.63      0.73       612\n",
      "           9       0.88      0.05      0.09       145\n",
      "          10       1.00      0.02      0.05        82\n",
      "          11       0.90      0.23      0.37       545\n",
      "          12       0.86      0.37      0.51       704\n",
      "          13       0.00      0.00      0.00        67\n",
      "          14       0.40      0.00      0.01       446\n",
      "          15       0.61      0.05      0.10       535\n",
      "          16       0.68      0.08      0.14       330\n",
      "          17       0.59      0.08      0.15       226\n",
      "          18       0.00      0.00      0.00        72\n",
      "          19       0.89      0.05      0.10       159\n",
      "          20       0.00      0.00      0.00        36\n",
      "          21       0.52      0.03      0.06       867\n",
      "          22       0.57      0.01      0.03       304\n",
      "          23       0.53      0.03      0.05       357\n",
      "          24       0.57      0.02      0.04       214\n",
      "          25       0.82      0.94      0.87      4985\n",
      "          26       0.83      0.35      0.49      1078\n",
      "          27       0.82      0.10      0.18       179\n",
      "          28       0.20      0.01      0.02       123\n",
      "          29       0.77      0.20      0.32       570\n",
      "          30       0.00      0.00      0.00        30\n",
      "          31       0.78      0.36      0.49       572\n",
      "          32       0.00      0.00      0.00        37\n",
      "          33       0.79      0.07      0.13       312\n",
      "          34       0.85      0.14      0.24       439\n",
      "          35       0.84      0.50      0.62      1815\n",
      "\n",
      "   micro avg       0.81      0.44      0.57     20723\n",
      "   macro avg       0.59      0.15      0.21     20723\n",
      "weighted avg       0.75      0.44      0.50     20723\n",
      " samples avg       0.67      0.42      0.47     20723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "#confusion_matrix(y_test,y_pred)\n",
    "#print(confusion_matrix)\n",
    "\n",
    "for columns in df.columns:\n",
    "    \n",
    "    print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "        'features__text_pipeline__vect__ngram_range': ((1, 1), (1, 2)),\n",
    "        'features__text_pipeline__vect__max_df': (0.5, 0.75, 1.0),\n",
    "        'features__text_pipeline__vect__max_features': (None, 5000, 10000),\n",
    "        'features__text_pipeline__tfidf__use_idf': (True, False),\n",
    "        'clf__n_estimators': [50, 100, 200],\n",
    "        'clf__min_samples_split': [2, 3, 4],\n",
    "        'features__transformer_weights': (\n",
    "            {'text_pipeline': 1, 'starting_verb': 0.5},\n",
    "            {'text_pipeline': 0.5, 'starting_verb': 1},\n",
    "            {'text_pipeline': 0.8, 'starting_verb': 1},\n",
    "        )\n",
    "    }\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (y_pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9419904723154647"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-e87a0ef9dbef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# get counts of each token (word) in text data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Downloads\\New folder\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1032\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1033\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\New folder\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    940\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\New folder\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    326\u001b[0m                                                tokenize)\n\u001b[0;32m    327\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 328\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\New folder\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\New folder\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    684\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# initialize count vectorizer object\n",
    "vect = CountVectorizer(tokenizer=tokenize)\n",
    "# get counts of each token (word) in text data\n",
    "X = vect.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-5238e41eaf4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Downloads\\New folder\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    945\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\New folder\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.externals import joblib \n",
    "  \n",
    "# Save the model as a pickle in a file \n",
    "joblib.dump(pipeline, 'filename.pkl') \n",
    "  \n",
    "# Load the model from the file \n",
    "pipeline_from_joblib = joblib.load('filename.pkl')  \n",
    "  \n",
    "# Use the loaded model to make predictions \n",
    "pipeline_from_joblib.predict(X_test) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
